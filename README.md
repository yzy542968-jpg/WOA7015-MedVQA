# WOA7015 â€“ Medical Visual Question Answering (MedVQA)

This repository contains the code for the WOA7015 Alternative Assignment:
a comparative study of baseline CNN-LSTM and advanced Vision-Language Models
(ViLT and BLIP) on medical VQA datasets.

## Models
- CNN-LSTM baseline
- ViLT (Vision-and-Language Transformer)
- BLIP (Bootstrapping Language-Image Pre-training)

## Datasets
- VQA-RAD
- PathVQA

## Evaluation
- Closed-ended: Accuracy
- Open-ended: Top-1 Accuracy, BLEU score

## How to Run
1. Open the notebook `MedVQA_Final.ipynb`
2. Install dependencies
3. Run cells sequentially

## Note
This code is provided for academic purposes for the WOA7015 course.
